{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create a simple QA(Question Answer) on Text File with following steps\n",
        "- upload file to openai storage\n",
        "- create own assistant\n",
        "- create thread\n",
        "- add messages to thread\n",
        "- create run\n",
        "- get result from opean ai"
      ],
      "metadata": {
        "id": "uDtkBt4gSHfy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6HXtmj0SnLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqyLUYg5sUhK",
        "outputId": "6d80b0aa-eeb2-4959-d650-ce5e420fc988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/227.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dependencies\n",
        "from openai import OpenAI\n"
      ],
      "metadata": {
        "id": "o2CUGsPdOx5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK0YPnzVsbhO",
        "outputId": "006eed8b-0e52-43b1-b29c-86e90d10040d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<openai.OpenAI at 0x7909889bd180>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# connect with openai\n",
        "client = OpenAI(api_key=api_key = \"api-id\")\n",
        "client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4vEodClsbbS",
        "outputId": "586fc24e-521e-4773-9aa8-2fba94c615d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileObject(id='file-qEkiCPGFrAS8OLc22fc8dsJN', bytes=5299, created_at=1709224785, filename='story.txt', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# upload file to openai storage\n",
        "# get file id\n",
        "uploaded_file = client.files.create(\n",
        "    file=open(\"story.txt\",'rb'),\n",
        "    purpose='assistants'\n",
        ")\n",
        "uploaded_file\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ_VjFJ4sbYY"
      },
      "outputs": [],
      "source": [
        "# create own assistant\n",
        "# mentioning file id and get assistant id\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Story helper\",\n",
        "    instructions=\"You are a motivator who answers the question based on the story file\",\n",
        "    tools=[{\"type\": \"retrieval\"}],\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    file_ids=[uploaded_file.id]\n",
        ")\n",
        "assistant\n",
        "\n",
        "# Assistant(id='asst_VaRmvfp30jPvLuVJgYzxGmXf',\n",
        "#           created_at=1709224786,\n",
        "#           description=None,\n",
        "#           file_ids=['file-qEkiCPGFrAS8OLc22fc8dsJN'],\n",
        "#           instructions='You are a motivator who answers the question based on the story file',\n",
        "#           metadata={}, model='gpt-4-turbo-preview',\n",
        "#           name='Story helper',\n",
        "#           object='assistant',\n",
        "#           tools=[ToolRetrieval(type='retrieval')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6BgrO7PsbVQ",
        "outputId": "47d1b31d-6b1b-4b29-d47c-4447ed9488e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Thread(id='thread_CMaQ6801HYvVyxdhVLBum6Ot', created_at=1709224874, metadata={}, object='thread')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to begin interaction we need to crate threads\n",
        "# get thread id\n",
        "thread = client.beta.threads.create()\n",
        "thread\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhWJG0VQsbPV",
        "outputId": "59f47f35-9340-4384-ff61-fbc34b41cfe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ThreadMessage(id='msg_LDlW9vX8791DfLAYv30YDvzQ', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Who is the hero of the story?'), type='text')], created_at=1709224950, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_CMaQ6801HYvVyxdhVLBum6Ot')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add message to thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Who is the hero of the story?\"\n",
        ")\n",
        "message\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n167t7_bvzWc"
      },
      "outputs": [],
      "source": [
        "# for the assistant to respond to the user we need to create the run\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id\n",
        ")\n",
        "run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LTxbm44dwa9G",
        "outputId": "e7d4933d-485b-4b86-b246-e0d75a5e89db"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'completed'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to check the status of run\n",
        "run = client.beta.threads.runs.retrieve(\n",
        "  thread_id=thread.id,\n",
        "  run_id=run.id\n",
        ")\n",
        "run.status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2gaGxZ8whye",
        "outputId": "a3f2fbc9-ba11-40e0-ca8e-19e2cd258739"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_3oc7BbjhcaqWuMhs83XReWz3', assistant_id='asst_VaRmvfp30jPvLuVJgYzxGmXf', content=[MessageContentText(text=Text(annotations=[], value='The hero of the story \"The Odyssey of Lumina: Illuminating Lives\" is Dr. Michael Greene. He is portrayed as a brilliant inventor who creates Lumina, a revolutionary product designed to enhance human potential and productivity. Despite facing challenges, criticism, and the unforeseen consequences of his invention, Dr. Greene remains committed to improving it for the betterment of society. His journey showcases resilience, adaptability, and an enduring commitment to the greater good, making him the unequivocal hero of the story.'), type='text')], created_at=1709225111, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_eoHeWhFTc1JJ1xVB0VjnGhMO', thread_id='thread_CMaQ6801HYvVyxdhVLBum6Ot'), ThreadMessage(id='msg_LDlW9vX8791DfLAYv30YDvzQ', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Who is the hero of the story?'), type='text')], created_at=1709224950, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_CMaQ6801HYvVyxdhVLBum6Ot')], object='list', first_id='msg_3oc7BbjhcaqWuMhs83XReWz3', last_id='msg_LDlW9vX8791DfLAYv30YDvzQ', has_more=False)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get messages/result from openai\n",
        "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhIPBLQ4wvc2",
        "outputId": "ef589a59-ec78-4ea5-c466-654fb1a1e0a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The hero of the story \"The Odyssey of Lumina: Illuminating Lives\" is Dr. Michael Greene. He is portrayed as a brilliant inventor who creates Lumina, a revolutionary product designed to enhance human potential and productivity. Despite facing challenges, criticism, and the unforeseen consequences of his invention, Dr. Greene remains committed to improving it for the betterment of society. His journey showcases resilience, adaptability, and an enduring commitment to the greater good, making him the unequivocal hero of the story.\n"
          ]
        }
      ],
      "source": [
        "# run when status is completed to get the response\n",
        "# from openai\n",
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status==\"completed\":\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "        latest_message = messages.data[0]\n",
        "        text = latest_message.content[0].text.value\n",
        "        print(text)\n",
        "        break;\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y13Gor4ZxLe3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}